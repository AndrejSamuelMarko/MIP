In 1947, Alan Turing, the father of Computer Science, remarked, "I believe that at the end of the century, the use of words and general educated opinion will have altered so much that one will be able to speak of machines thinking without expecting to be contradicted." And it would not be a stretch to say that he was right. Such is the nature of discovery, of the hitherto considered unknown becoming the norm, of the old making way for the new that it is almost unfathomable to begin with.

While the notion of thinking machines dates back to centuries, albeit in folklore and anecdotes, the coinage of the term AI can be traced back to the 1950s. In the almost seven decades since, Artificial Intelligence technology has developed and evolved in several ways, as have its applications. 

 

The period between the 1950s and the 1970s in the history of AI revolved around the research on neural networks; the ensuing three decades (1980s to 2010s) were the dawn of the applications of Machine Learning. With consistent research, an ever-rising degree of intrigue, and widespread application, Machine Learning has made way for the subtler concept of Deep Learning. And the leap into the unknown that characterized the initial research in AI has become more of a leap of faith, with new chapters unfolding every year. 
